{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import minmax_scale as mms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,name):\n",
    "    csv_path = os.path.join(path, name)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wine dataset ...\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/DanielaFe7/Desktop/Maestrado/Redes Neurais/RedesNeurais\"\n",
    "name = \"winequality-red.csv\"\n",
    "data = load_data(path,name)\n",
    "print(\"Loading wine dataset ...\")\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert Categorical Output to One Hot Vector\n"
     ]
    }
   ],
   "source": [
    "#Convert Categorical Output to One Hot Vector: [100-010-001]\n",
    "print(\"Convert Categorical Output to One Hot Vector\")\n",
    "Y = data[\"category\"]\n",
    "Y = Y.values.reshape(-1,1)\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(Y)\n",
    "Y = enc.transform(Y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing inputs droping Labels and droping indexs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  \n",
       "0      9.4  \n",
       "1      9.8  \n",
       "2      9.8  \n",
       "3      9.8  \n",
       "4      9.4  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data\n",
    "X = X.drop(\"Unnamed: 0\", axis = 1)\n",
    "X = X.drop(\"category\", axis = 1)\n",
    "print(\"Preparing inputs droping Labels and droping indexs\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traind and Test datasets Stratify Splitting\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=0, stratify=Y)\n",
    "print(\"Traind and Test datasets Stratify Splitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test Shapes after Splitting\n",
      "(1199, 11)\n",
      "(400, 11)\n",
      "(1199, 3)\n",
      "(400, 3)\n",
      "Train and Test Shapes after Scalling : Values between 0 and 1\n",
      "(1199, 11)\n",
      "(400, 11)\n",
      "(1199, 3)\n",
      "(400, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train and Test Shapes after Splitting\")\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(\"Train and Test Shapes after Scalling : Values between 0 and 1\")\n",
    "x_train = mms(x_train)\n",
    "x_test = mms(x_test)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "#x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN L-Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    S = 1 / (1 + np.exp(-Z))\n",
    "    return S,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivade_sigmoid(z):\n",
    "    gz,z = sigmoid(z) \n",
    "    return gz * (1-gz);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network, (including input layer)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.normal(0, 1, (layer_dims[l], layer_dims[l-1]))\n",
    "        parameters['b' + str(l)] = np.random.random((layer_dims[l], 1))\n",
    "      \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation in L - Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache) #linear_cahce: A_prev,Wi,bi - activation_cache: Zi\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X,Y,parameters,pred=False):\n",
    "    caches = []\n",
    "    A = X                                     #(input size, number of examples)\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network (not input layer including)\n",
    "    \n",
    "    # Implement [LINEAR -> SIGMOID]*(L-1). To L-1 Layers\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        W=parameters['W' + str(l)]\n",
    "        b=parameters['b' + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, \"sigmoid\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Last layer\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    # Predicction\n",
    "    if pred:\n",
    "        # The max value is 1 , else 0\n",
    "        AL_temp = AL.T\n",
    "        Y_prediction = np.zeros_like(AL_temp)\n",
    "        Y_prediction[np.arange(len(AL_temp)), AL_temp.argmax(1)] = 1\n",
    "        Y_prediction = Y_prediction.T\n",
    "        return Y_prediction\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (- 1 / m) * np.sum(Y * np.log(AL) + (1 - Y) * (np.log(1 - AL)))\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation in L - Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m) * np.dot(dZ,A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = dA * derivade_sigmoid(activation_cache) # activation_cache = Z ; dA = np.dot(W.T,dZ) excep the first dA\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers (not input layer including)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL =- (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, \n",
    "                                                                                                  current_cache, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (SIGMOID -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev, dW, db = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"sigmoid\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev\n",
    "        grads[\"dW\" + str(l + 1)] = dW\n",
    "        grads[\"db\" + str(l + 1)] = db\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network (not input layer including)\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] =parameters[\"W\" + str(l+1)]-learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] =parameters[\"b\" + str(l+1)]-learning_rate*grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L - layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.5, num_iterations = 3000, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []                         \n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> SIGMOID]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X,Y,parameters,pred=False)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # Predicction\n",
    "    Y_prediction =  L_model_forward(X,Y,parameters,pred=True)\n",
    "    print(\"------------------\")\n",
    "    print(\"Prediccion:\")\n",
    "    print(Y_prediction.shape)\n",
    "    print(Y_prediction)\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction - Y)) * 100))\n",
    "    print(\"F1 Score: \",f1_score(Y, Y_prediction, average='macro')*100)  \n",
    "    print(\"Accuracy: \",accuracy_score(Y, Y_prediction)*100) \n",
    "\n",
    "\n",
    "    if np.array_equal(Y,Y_prediction):\n",
    "        print(\"Success Prediction\")\n",
    "    else:\n",
    "        print(\"Un - success Prediction\")\n",
    "        \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "            \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1199)\n",
      "(3, 1199)\n",
      "Cost after iteration 0: 1.961247\n",
      "Cost after iteration 100: 1.015199\n",
      "Cost after iteration 200: 0.996755\n",
      "Cost after iteration 300: 0.952776\n",
      "Cost after iteration 400: 0.882456\n",
      "Cost after iteration 500: 0.842637\n",
      "Cost after iteration 600: 0.831217\n",
      "Cost after iteration 700: 0.825517\n",
      "Cost after iteration 800: 0.821289\n",
      "Cost after iteration 900: 0.817874\n",
      "Cost after iteration 1000: 0.815033\n",
      "Cost after iteration 1100: 0.812609\n",
      "Cost after iteration 1200: 0.810491\n",
      "Cost after iteration 1300: 0.808596\n",
      "Cost after iteration 1400: 0.806866\n",
      "Cost after iteration 1500: 0.805254\n",
      "Cost after iteration 1600: 0.803726\n",
      "Cost after iteration 1700: 0.802255\n",
      "Cost after iteration 1800: 0.800819\n",
      "Cost after iteration 1900: 0.799402\n",
      "Cost after iteration 2000: 0.797989\n",
      "Cost after iteration 2100: 0.796569\n",
      "Cost after iteration 2200: 0.795131\n",
      "Cost after iteration 2300: 0.793666\n",
      "Cost after iteration 2400: 0.792167\n",
      "Cost after iteration 2500: 0.790626\n",
      "Cost after iteration 2600: 0.789037\n",
      "Cost after iteration 2700: 0.787394\n",
      "Cost after iteration 2800: 0.785693\n",
      "Cost after iteration 2900: 0.783931\n",
      "Cost after iteration 3000: 0.782110\n",
      "Cost after iteration 3100: 0.783789\n",
      "Cost after iteration 3200: 0.784362\n",
      "Cost after iteration 3300: 0.783401\n",
      "Cost after iteration 3400: 0.781904\n",
      "Cost after iteration 3500: 0.780426\n",
      "Cost after iteration 3600: 0.778981\n",
      "Cost after iteration 3700: 0.777546\n",
      "Cost after iteration 3800: 0.776115\n",
      "Cost after iteration 3900: 0.774691\n",
      "Cost after iteration 4000: 0.773278\n",
      "Cost after iteration 4100: 0.771880\n",
      "Cost after iteration 4200: 0.770501\n",
      "Cost after iteration 4300: 0.769146\n",
      "Cost after iteration 4400: 0.767816\n",
      "Cost after iteration 4500: 0.766516\n",
      "Cost after iteration 4600: 0.765245\n",
      "Cost after iteration 4700: 0.764006\n",
      "Cost after iteration 4800: 0.762799\n",
      "Cost after iteration 4900: 0.761622\n",
      "------------------\n",
      "Prediccion:\n",
      "(3, 1199)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [1. 1. 1. ... 1. 0. 1.]]\n",
      "train accuracy: 89.71365026410898 %\n",
      "F1 Score:  84.57047539616347\n",
      "Accuracy:  0.0\n",
      "Un - success Prediction\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = x_train.T\n",
    "Y = y_train.T\n",
    "#print(X)\n",
    "#print(Y)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "layers_dims = [11, 9, 7, 5 ,3] #  2-layer model\n",
    "parameters = L_layer_model(X, Y, layers_dims, num_iterations = 5000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
