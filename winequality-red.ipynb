{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import minmax_scale as mms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,name):\n",
    "    csv_path = os.path.join(path, name)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wine dataset ...\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/DanielaFe7/Desktop/Maestrado/Redes Neurais/RedesNeurais\"\n",
    "name = \"winequality-red.csv\"\n",
    "data = load_data(path,name)\n",
    "print(\"Loading wine dataset ...\")\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert Categorical Output to One Hot Vector\n"
     ]
    }
   ],
   "source": [
    "#Convert Categorical Output to One Hot Vector: [100-010-001]\n",
    "print(\"Convert Categorical Output to One Hot Vector\")\n",
    "Y = data[\"category\"]\n",
    "Y = Y.values.reshape(-1,1)\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(Y)\n",
    "Y = enc.transform(Y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing inputs droping Labels and droping indexs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  \n",
       "0      9.4  \n",
       "1      9.8  \n",
       "2      9.8  \n",
       "3      9.8  \n",
       "4      9.4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data\n",
    "X = X.drop(\"Unnamed: 0\", axis = 1)\n",
    "X = X.drop(\"category\", axis = 1)\n",
    "print(\"Preparing inputs droping Labels and droping indexs\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traind and Test datasets Stratify Splitting\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=0, stratify=Y)\n",
    "print(\"Traind and Test datasets Stratify Splitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test Shapes after Splitting\n",
      "(1199, 11)\n",
      "(400, 11)\n",
      "(1199, 3)\n",
      "(400, 3)\n",
      "Train and Test Shapes after Scalling : Values between 0 and 1\n",
      "(1199, 11)\n",
      "(400, 11)\n",
      "(1199, 3)\n",
      "(400, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train and Test Shapes after Splitting\")\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(\"Train and Test Shapes after Scalling : Values between 0 and 1\")\n",
    "x_train = mms(x_train)\n",
    "x_test = mms(x_test)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "#x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN L-Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    S = 1 / (1 + np.exp(-Z))\n",
    "    return S,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivade_sigmoid(z):\n",
    "    gz,z = sigmoid(z) \n",
    "    return gz * (1-gz);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network, (including input layer)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.normal(0, 1, (layer_dims[l], layer_dims[l-1]))\n",
    "        parameters['b' + str(l)] = np.random.random((layer_dims[l], 1))\n",
    "      \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks (not input layer including)\n",
    "    v = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l+1)])\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l+1)])\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation in L - Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache) #linear_cahce: A_prev,Wi,bi - activation_cache: Zi\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X,Y,parameters,pred=False):\n",
    "    caches = []\n",
    "    A = X                                     #(input size, number of examples)\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network (not input layer including)\n",
    "    \n",
    "    # Implement [LINEAR -> SIGMOID]*(L-1). To L-1 Layers\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        W=parameters['W' + str(l)]\n",
    "        b=parameters['b' + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, \"sigmoid\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Last layer\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    # Predicction\n",
    "    if pred:\n",
    "        # The max value is 1 , else 0\n",
    "        AL_temp = AL.T\n",
    "        Y_prediction = np.zeros_like(AL_temp)\n",
    "        Y_prediction[np.arange(len(AL_temp)), AL_temp.argmax(1)] = 1\n",
    "        Y_prediction = Y_prediction.T\n",
    "        return Y_prediction\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (- 1 / m) * np.sum(Y * np.log(AL) + (1 - Y) * (np.log(1 - AL)))\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation in L - Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m) * np.dot(dZ,A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = dA * derivade_sigmoid(activation_cache) # activation_cache = Z ; dA = np.dot(W.T,dZ) excep the first dA\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers (not input layer including)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL =- (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, \n",
    "                                                                                                  current_cache, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (SIGMOID -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev, dW, db = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"sigmoid\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev\n",
    "        grads[\"dW\" + str(l + 1)] = dW\n",
    "        grads[\"db\" + str(l + 1)] = db\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network (not input layer including)\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Parameters with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, learning_rate, beta):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks (not input layer including)\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads['dW' + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads['db' + str(l + 1)]\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L - layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, X_test, Y_test, layers_dims, learning_rate, num_iterations, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []                         \n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    #Velocity initialization.\n",
    "    v = initialize_velocity(parameters)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> SIGMOID]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X,Y,parameters,pred=False)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters. #Esto tem que quedar comentado si vc vai usar com Momemtum\n",
    "        #parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Update parameters with Momentum.\n",
    "        parameters, v = update_parameters_with_momentum(parameters, grads, v, learning_rate, beta = 0.9)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 500 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 500 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # Train Predicction\n",
    "    Y_prediction =  L_model_forward(X,Y,parameters,pred=True)\n",
    "    print(\"------------------\")\n",
    "    print(\"Train Prediccion:\")\n",
    "    print(Y_prediction.shape)\n",
    "    print(Y_prediction)\n",
    "    print(\"Train Accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction - Y)) * 100))\n",
    "    print(\"Train F1 Score: \",f1_score(Y, Y_prediction, average='macro')*100)  \n",
    "    \n",
    "    # Test Predicction\n",
    "    print(\"------------------\")\n",
    "    Y_prediction =  L_model_forward(X_test,Y_test,parameters,pred=True)\n",
    "    print(\"Test Prediccion:\")\n",
    "    print(Y_prediction.shape)\n",
    "    print(Y_prediction)\n",
    "    print(\"Test Accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction - Y_test)) * 100))\n",
    "    print(\"Test F1 Score: \",f1_score(Y_test, Y_prediction, average='macro')*100)  \n",
    "\n",
    "\n",
    "\n",
    "    if np.array_equal(Y,Y_prediction):\n",
    "        print(\"Success Prediction\")\n",
    "    else:\n",
    "        print(\"Un - success Prediction\")\n",
    "        \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "            \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1199)\n",
      "(3, 1199)\n",
      "Cost after iteration 0: 1.961247\n",
      "Cost after iteration 500: 0.845918\n",
      "Cost after iteration 1000: 0.815891\n",
      "Cost after iteration 1500: 0.805659\n",
      "Cost after iteration 2000: 0.798088\n",
      "Cost after iteration 2500: 0.790639\n",
      "Cost after iteration 3000: 0.782327\n",
      "Cost after iteration 3500: 0.772807\n",
      "Cost after iteration 4000: 0.762725\n",
      "Cost after iteration 4500: 0.752688\n",
      "Cost after iteration 5000: 0.742166\n",
      "Cost after iteration 5500: 0.730470\n",
      "Cost after iteration 6000: 0.718244\n",
      "Cost after iteration 6500: 0.707193\n",
      "Cost after iteration 7000: 0.698066\n",
      "Cost after iteration 7500: 0.690675\n",
      "Cost after iteration 8000: 0.684602\n",
      "Cost after iteration 8500: 0.679359\n",
      "Cost after iteration 9000: 0.674486\n",
      "Cost after iteration 9500: 0.669678\n",
      "------------------\n",
      "Train Prediccion:\n",
      "(3, 1199)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [1. 1. 1. ... 1. 0. 1.]]\n",
      "Train Accuracy: 90.99249374478732 %\n",
      "Train F1 Score:  86.48874061718098\n",
      "------------------\n",
      "Test Prediccion:\n",
      "(3, 400)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [1. 1. 1. ... 0. 1. 1.]]\n",
      "Test Accuracy: 89.16666666666667 %\n",
      "Test F1 Score:  83.75\n",
      "Un - success Prediction\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, Y = x_train.T , y_train.T\n",
    "X_test, Y_test = x_test.T, y_test.T \n",
    "#print(X)\n",
    "#print(Y)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "layers_dims = [11, 9, 7, 5 ,3] #  2-layer model\n",
    "parameters = L_layer_model(X, Y, X_test, Y_test,layers_dims, learning_rate = 0.5, num_iterations = 10000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
