{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import minmax_scale as mms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,name):\n",
    "    csv_path = os.path.join(path, name)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wine dataset ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "0           0            7.4              0.70         0.00             1.9   \n",
       "1           1            7.8              0.88         0.00             2.6   \n",
       "2           2            7.8              0.76         0.04             2.3   \n",
       "3           3           11.2              0.28         0.56             1.9   \n",
       "4           4            7.4              0.70         0.00             1.9   \n",
       "\n",
       "   chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "0      0.076                 11.0                  34.0   0.9978  3.51   \n",
       "1      0.098                 25.0                  67.0   0.9968  3.20   \n",
       "2      0.092                 15.0                  54.0   0.9970  3.26   \n",
       "3      0.075                 17.0                  60.0   0.9980  3.16   \n",
       "4      0.076                 11.0                  34.0   0.9978  3.51   \n",
       "\n",
       "   sulphates  alcohol category  \n",
       "0       0.56      9.4      Mid  \n",
       "1       0.68      9.8      Mid  \n",
       "2       0.65      9.8      Mid  \n",
       "3       0.58      9.8      Mid  \n",
       "4       0.56      9.4      Mid  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"C:/Users/DanielaFe7/Desktop/Maestrado/Redes/RedesNeurais\"\n",
    "name = \"winequality-red.csv\"\n",
    "data = load_data(path,name)\n",
    "print(\"Loading wine dataset ...\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert Categorical Output to One Hot Vector\n"
     ]
    }
   ],
   "source": [
    "print(\"Convert Categorical Output to One Hot Vector\")\n",
    "def categorical_output(Y_index): \n",
    "    #Y = data[\"category\"]\n",
    "    Y = Y_index.values.reshape(-1,1)\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(Y)\n",
    "    Y = enc.transform(Y).toarray() #Converting in hot vectors\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing inputs droping Labels and droping indexs\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing inputs droping Labels and droping indexs\")\n",
    "def preprocessing_input(X_index):\n",
    "    #X = data\n",
    "    #X = X_index.drop(\"Unnamed: 0\", axis = 1)\n",
    "    #X = X_index.drop(\"category\", axis = 1)\n",
    "    X = mms(X_index) #Scalling \n",
    "    return X\n",
    "    #X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN L-Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    S = 1 / (1 + np.exp(-Z))\n",
    "    return S,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivade_sigmoid(z):\n",
    "    gz,z = sigmoid(z) \n",
    "    return gz * (1-gz);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network, (including input layer)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.normal(0, 1, (layer_dims[l], layer_dims[l-1]))\n",
    "        parameters['b' + str(l)] = np.random.random((layer_dims[l], 1))\n",
    "      \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks (not input layer including)\n",
    "    v = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l+1)])\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l+1)])\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y_pred, Y):\n",
    "    contador = 0\n",
    "    Y_pred = Y_pred.T\n",
    "    Y = Y.T\n",
    "    for i in range(Y_pred.shape[0]) :\n",
    "        if np.array_equal(Y_pred[i],Y[i]):\n",
    "            contador +=1\n",
    "    accuracy = (contador*100.0)/Y_pred.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation in L - Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache) #linear_cahce: A_prev,Wi,bi - activation_cache: Zi\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X,Y,parameters,pred=False):\n",
    "    caches = []\n",
    "    A = X                                     #(input size, number of examples)\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network (not input layer including)\n",
    "    \n",
    "    # Implement [LINEAR -> SIGMOID]*(L-1). To L-1 Layers\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        W=parameters['W' + str(l)]\n",
    "        b=parameters['b' + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, \"sigmoid\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Last layer\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    # Predicction\n",
    "    if pred:\n",
    "        # The max value is 1 , else 0\n",
    "        AL_temp = AL.T\n",
    "        Y_prediction = np.zeros_like(AL_temp)\n",
    "        Y_prediction[np.arange(len(AL_temp)), AL_temp.argmax(1)] = 1\n",
    "        Y_prediction = Y_prediction.T\n",
    "        return Y_prediction\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (- 1 / m) * np.sum(Y * np.log(AL) + (1 - Y) * (np.log(1 - AL)))\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation in L - Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m) * np.dot(dZ,A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = dA * derivade_sigmoid(activation_cache) # activation_cache = Z ; dA = np.dot(W.T,dZ) excep the first dA\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers (not input layer including)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL =- (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, \n",
    "                                                                                                  current_cache, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (SIGMOID -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev, dW, db = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"sigmoid\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev\n",
    "        grads[\"dW\" + str(l + 1)] = dW\n",
    "        grads[\"db\" + str(l + 1)] = db\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network (not input layer including)\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Parameters with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, learning_rate, beta):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks (not input layer including)\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads['dW' + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads['db' + str(l + 1)]\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L - layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = []\n",
    "def L_layer_model(X, Y, X_test, Y_test, layers_dims, learning_rate, num_iterations, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []                         \n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    #Velocity initialization.\n",
    "    v = initialize_velocity(parameters)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> SIGMOID]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X,Y,parameters,pred=False)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters. #Esto tem que quedar comentado si vc vai usar com Momemtum\n",
    "        #parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Update parameters with Momentum.\n",
    "        parameters, v = update_parameters_with_momentum(parameters, grads, v, learning_rate, beta = 0.99)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 500 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 500 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # Train Predicction\n",
    "    Y_prediction =  L_model_forward(X,Y,parameters,pred=True)\n",
    "    print(\"------------------\")\n",
    "    #print(\"Train Prediccion:\")\n",
    "    #print(Y_prediction.shape)\n",
    "    #print(Y_prediction)\n",
    "    print(\"Train Accuracy: \",accuracy(Y_prediction, Y))  \n",
    "    \n",
    "    # Test Predicction\n",
    "    print(\"------------------\")\n",
    "    Y_prediction =  L_model_forward(X_test,Y_test,parameters,pred=True)\n",
    "    #print(\"Test Prediccion:\")\n",
    "    #print(Y_prediction.shape)\n",
    "    #print(Y_prediction)\n",
    "    print(\"Test Accuracy: \",accuracy(Y_prediction, Y_test))  \n",
    "      \n",
    "    # plot the cost\n",
    "    #plt.plot(np.squeeze(costs))\n",
    "    #plt.ylabel('cost')\n",
    "    #plt.xlabel('iterations (per tens)')\n",
    "    #plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    #plt.show()\n",
    "            \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation : StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=2, shuffle=True)\n",
      "(1599, 11)\n",
      "(1599, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "# We can use StratifiedKFold because is an Classification problem\n",
    "Y_index = data[\"category\"]\n",
    "X_index = data\n",
    "X_index = X_index.drop(\"Unnamed: 0\", axis = 1)\n",
    "X_index = X_index.drop(\"category\", axis = 1)\n",
    "skf = StratifiedKFold(n_splits=10,random_state=2,shuffle=True)\n",
    "skf.get_n_splits(X_index, Y_index)\n",
    "print(skf)\n",
    "\n",
    "# When the indexes are ready in (split function), pick the data that is already pre-processing\n",
    "XX = data.drop(\"Unnamed: 0\", axis = 1)\n",
    "XX = XX.drop(\"category\", axis = 1)\n",
    "XX = mms(XX)\n",
    "\n",
    "YY = data[\"category\"]\n",
    "YY = categorical_output(YY)\n",
    "print(XX.shape)\n",
    "print(YY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "(1438, 11)\n",
      "(1438, 3)\n",
      "(161, 11)\n",
      "(161, 3)\n",
      "Cost after iteration 0: 7.510584\n",
      "Cost after iteration 500: 1.027898\n",
      "Cost after iteration 1000: 1.024486\n",
      "Cost after iteration 1500: 1.024092\n",
      "Cost after iteration 2000: 1.020367\n",
      "Cost after iteration 2500: 0.844679\n",
      "Cost after iteration 3000: 0.816127\n",
      "Cost after iteration 3500: 0.811492\n",
      "Cost after iteration 4000: 0.807857\n",
      "Cost after iteration 4500: 0.803279\n",
      "Cost after iteration 5000: 0.797331\n",
      "Cost after iteration 5500: 0.791358\n",
      "Cost after iteration 6000: 0.785431\n",
      "Cost after iteration 6500: 0.778346\n",
      "Cost after iteration 7000: 0.770158\n",
      "Cost after iteration 7500: 0.762617\n",
      "Cost after iteration 8000: 0.756520\n",
      "Cost after iteration 8500: 0.750627\n",
      "Cost after iteration 9000: 0.744451\n",
      "Cost after iteration 9500: 0.737658\n",
      "Cost after iteration 10000: 0.729785\n",
      "Cost after iteration 10500: 0.723739\n",
      "Cost after iteration 11000: 0.718992\n",
      "Cost after iteration 11500: 0.714935\n",
      "Cost after iteration 12000: 0.711305\n",
      "Cost after iteration 12500: 0.707932\n",
      "Cost after iteration 13000: 0.704656\n",
      "Cost after iteration 13500: 0.701356\n",
      "Cost after iteration 14000: 0.697965\n",
      "Cost after iteration 14500: 0.694416\n",
      "Cost after iteration 15000: 0.690607\n",
      "Cost after iteration 15500: 0.686495\n",
      "Cost after iteration 16000: 0.682122\n",
      "Cost after iteration 16500: 0.677549\n",
      "Cost after iteration 17000: 0.672920\n",
      "Cost after iteration 17500: 0.668387\n",
      "Cost after iteration 18000: 0.663962\n",
      "Cost after iteration 18500: 0.659510\n",
      "Cost after iteration 19000: 0.654913\n",
      "Cost after iteration 19500: 0.650054\n",
      "------------------\n",
      "Train Accuracy:  89.221140472879\n",
      "------------------\n",
      "Test Accuracy:  82.6086956521739\n",
      "------------------\n",
      "Fold:  2\n",
      "(1438, 11)\n",
      "(1438, 3)\n",
      "(161, 11)\n",
      "(161, 3)\n",
      "Cost after iteration 0: 7.510160\n",
      "Cost after iteration 500: 1.027949\n",
      "Cost after iteration 1000: 1.024470\n",
      "Cost after iteration 1500: 1.023985\n",
      "Cost after iteration 2000: 1.015069\n",
      "Cost after iteration 2500: 0.837169\n",
      "Cost after iteration 3000: 0.813442\n",
      "Cost after iteration 3500: 0.807477\n",
      "Cost after iteration 4000: 0.802343\n",
      "Cost after iteration 4500: 0.797467\n",
      "Cost after iteration 5000: 0.792913\n",
      "Cost after iteration 5500: 0.788107\n",
      "Cost after iteration 6000: 0.781877\n",
      "Cost after iteration 6500: 0.773384\n",
      "Cost after iteration 7000: 0.764221\n",
      "Cost after iteration 7500: 0.757372\n",
      "Cost after iteration 8000: 0.751684\n",
      "Cost after iteration 8500: 0.746837\n",
      "Cost after iteration 9000: 0.742414\n",
      "Cost after iteration 9500: 0.738043\n",
      "Cost after iteration 10000: 0.733687\n",
      "Cost after iteration 10500: 0.729312\n",
      "Cost after iteration 11000: 0.724923\n",
      "Cost after iteration 11500: 0.720438\n",
      "Cost after iteration 12000: 0.715046\n",
      "Cost after iteration 12500: 0.710855\n",
      "Cost after iteration 13000: 0.707156\n",
      "Cost after iteration 13500: 0.703484\n",
      "Cost after iteration 14000: 0.699695\n",
      "Cost after iteration 14500: 0.695663\n",
      "Cost after iteration 15000: 0.691563\n",
      "Cost after iteration 15500: 0.687568\n",
      "Cost after iteration 16000: 0.683487\n",
      "Cost after iteration 16500: 0.679109\n",
      "Cost after iteration 17000: 0.674361\n",
      "Cost after iteration 17500: 0.668997\n",
      "Cost after iteration 18000: 0.661650\n",
      "Cost after iteration 18500: 0.654414\n",
      "Cost after iteration 19000: 0.648728\n",
      "Cost after iteration 19500: 0.644009\n",
      "------------------\n",
      "Train Accuracy:  90.12517385257301\n",
      "------------------\n",
      "Test Accuracy:  85.09316770186335\n",
      "------------------\n",
      "Fold:  3\n",
      "(1438, 11)\n",
      "(1438, 3)\n",
      "(161, 11)\n",
      "(161, 3)\n",
      "Cost after iteration 0: 7.511683\n",
      "Cost after iteration 500: 1.027903\n",
      "Cost after iteration 1000: 1.024487\n",
      "Cost after iteration 1500: 1.024093\n",
      "Cost after iteration 2000: 1.020470\n",
      "Cost after iteration 2500: 0.857844\n",
      "Cost after iteration 3000: 0.826232\n",
      "Cost after iteration 3500: 0.821843\n",
      "Cost after iteration 4000: 0.818962\n",
      "Cost after iteration 4500: 0.815690\n",
      "Cost after iteration 5000: 0.811136\n",
      "Cost after iteration 5500: 0.805272\n",
      "Cost after iteration 6000: 0.798721\n",
      "Cost after iteration 6500: 0.792454\n",
      "Cost after iteration 7000: 0.784591\n",
      "Cost after iteration 7500: 0.774409\n",
      "Cost after iteration 8000: 0.765918\n",
      "Cost after iteration 8500: 0.759947\n",
      "Cost after iteration 9000: 0.754647\n",
      "Cost after iteration 9500: 0.749610\n",
      "Cost after iteration 10000: 0.744700\n",
      "Cost after iteration 10500: 0.739675\n",
      "Cost after iteration 11000: 0.734348\n",
      "Cost after iteration 11500: 0.728675\n",
      "Cost after iteration 12000: 0.722709\n",
      "Cost after iteration 12500: 0.716558\n",
      "Cost after iteration 13000: 0.710456\n",
      "Cost after iteration 13500: 0.705193\n",
      "Cost after iteration 14000: 0.700435\n",
      "Cost after iteration 14500: 0.695948\n",
      "Cost after iteration 15000: 0.691620\n",
      "Cost after iteration 15500: 0.687428\n",
      "Cost after iteration 16000: 0.683451\n",
      "Cost after iteration 16500: 0.679798\n",
      "Cost after iteration 17000: 0.676489\n",
      "Cost after iteration 17500: 0.673459\n",
      "Cost after iteration 18000: 0.670663\n",
      "Cost after iteration 18500: 0.668084\n",
      "Cost after iteration 19000: 0.665697\n",
      "Cost after iteration 19500: 0.663461\n",
      "------------------\n",
      "Train Accuracy:  88.45618915159945\n",
      "------------------\n",
      "Test Accuracy:  86.33540372670808\n",
      "------------------\n",
      "Fold:  4\n",
      "(1439, 11)\n",
      "(1439, 3)\n",
      "(160, 11)\n",
      "(160, 3)\n",
      "Cost after iteration 0: 7.508122\n",
      "Cost after iteration 500: 1.029065\n",
      "Cost after iteration 1000: 1.027249\n",
      "Cost after iteration 1500: 1.026682\n",
      "Cost after iteration 2000: 1.008543\n",
      "Cost after iteration 2500: 0.835928\n",
      "Cost after iteration 3000: 0.819244\n",
      "Cost after iteration 3500: 0.811793\n",
      "Cost after iteration 4000: 0.805021\n",
      "Cost after iteration 4500: 0.800353\n",
      "Cost after iteration 5000: 0.796738\n",
      "Cost after iteration 5500: 0.793273\n",
      "Cost after iteration 6000: 0.789434\n",
      "Cost after iteration 6500: 0.783741\n",
      "Cost after iteration 7000: 0.774460\n",
      "Cost after iteration 7500: 0.767355\n",
      "Cost after iteration 8000: 0.762627\n",
      "Cost after iteration 8500: 0.757948\n",
      "Cost after iteration 9000: 0.752826\n",
      "Cost after iteration 9500: 0.747873\n",
      "Cost after iteration 10000: 0.743479\n",
      "Cost after iteration 10500: 0.739587\n",
      "Cost after iteration 11000: 0.735999\n",
      "Cost after iteration 11500: 0.732581\n",
      "Cost after iteration 12000: 0.729254\n",
      "Cost after iteration 12500: 0.725960\n",
      "Cost after iteration 13000: 0.722642\n",
      "Cost after iteration 13500: 0.719231\n",
      "Cost after iteration 14000: 0.715638\n",
      "Cost after iteration 14500: 0.711760\n",
      "Cost after iteration 15000: 0.707509\n",
      "Cost after iteration 15500: 0.702869\n",
      "Cost after iteration 16000: 0.698013\n",
      "Cost after iteration 16500: 0.693267\n",
      "Cost after iteration 17000: 0.688830\n",
      "Cost after iteration 17500: 0.684720\n",
      "Cost after iteration 18000: 0.680800\n",
      "Cost after iteration 18500: 0.676960\n",
      "Cost after iteration 19000: 0.673330\n",
      "Cost after iteration 19500: 0.670011\n",
      "------------------\n",
      "Train Accuracy:  89.71507991660876\n",
      "------------------\n",
      "Test Accuracy:  83.125\n",
      "------------------\n",
      "Fold:  5\n",
      "(1439, 11)\n",
      "(1439, 3)\n",
      "(160, 11)\n",
      "(160, 3)\n",
      "Cost after iteration 0: 7.507794\n",
      "Cost after iteration 500: 1.029056\n",
      "Cost after iteration 1000: 1.027235\n",
      "Cost after iteration 1500: 1.026599\n",
      "Cost after iteration 2000: 1.000794\n",
      "Cost after iteration 2500: 0.814643\n",
      "Cost after iteration 3000: 0.798573\n",
      "Cost after iteration 3500: 0.793118\n",
      "Cost after iteration 4000: 0.788158\n",
      "Cost after iteration 4500: 0.782854\n",
      "Cost after iteration 5000: 0.777104\n",
      "Cost after iteration 5500: 0.770807\n",
      "Cost after iteration 6000: 0.763836\n",
      "Cost after iteration 6500: 0.756741\n",
      "Cost after iteration 7000: 0.750487\n",
      "Cost after iteration 7500: 0.745553\n",
      "Cost after iteration 8000: 0.741573\n",
      "Cost after iteration 8500: 0.738086\n",
      "Cost after iteration 9000: 0.734721\n",
      "Cost after iteration 9500: 0.731225\n",
      "Cost after iteration 10000: 0.727490\n",
      "Cost after iteration 10500: 0.723538\n",
      "Cost after iteration 11000: 0.719440\n",
      "Cost after iteration 11500: 0.715247\n",
      "Cost after iteration 12000: 0.710904\n",
      "Cost after iteration 12500: 0.706310\n",
      "Cost after iteration 13000: 0.701504\n",
      "Cost after iteration 13500: 0.696520\n",
      "Cost after iteration 14000: 0.691276\n",
      "Cost after iteration 14500: 0.685758\n",
      "Cost after iteration 15000: 0.679913\n",
      "Cost after iteration 15500: 0.673296\n",
      "Cost after iteration 16000: 0.664906\n",
      "Cost after iteration 16500: 0.656628\n",
      "Cost after iteration 17000: 0.649432\n",
      "Cost after iteration 17500: 0.642533\n",
      "Cost after iteration 18000: 0.635015\n",
      "Cost after iteration 18500: 0.627013\n",
      "Cost after iteration 19000: 0.619754\n",
      "Cost after iteration 19500: 0.613148\n",
      "------------------\n",
      "Train Accuracy:  89.50660180681028\n",
      "------------------\n",
      "Test Accuracy:  81.25\n",
      "------------------\n",
      "Fold:  6\n",
      "(1439, 11)\n",
      "(1439, 3)\n",
      "(160, 11)\n",
      "(160, 3)\n",
      "Cost after iteration 0: 7.507634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 500: 1.029067\n",
      "Cost after iteration 1000: 1.027232\n",
      "Cost after iteration 1500: 1.026546\n",
      "Cost after iteration 2000: 0.996469\n",
      "Cost after iteration 2500: 0.828330\n",
      "Cost after iteration 3000: 0.813851\n",
      "Cost after iteration 3500: 0.809417\n",
      "Cost after iteration 4000: 0.805713\n",
      "Cost after iteration 4500: 0.801534\n",
      "Cost after iteration 5000: 0.796528\n",
      "Cost after iteration 5500: 0.790526\n",
      "Cost after iteration 6000: 0.782941\n",
      "Cost after iteration 6500: 0.773827\n",
      "Cost after iteration 7000: 0.765990\n",
      "Cost after iteration 7500: 0.760212\n",
      "Cost after iteration 8000: 0.754941\n",
      "Cost after iteration 8500: 0.749557\n",
      "Cost after iteration 9000: 0.744231\n",
      "Cost after iteration 9500: 0.739094\n",
      "Cost after iteration 10000: 0.733931\n",
      "Cost after iteration 10500: 0.728363\n",
      "Cost after iteration 11000: 0.721493\n",
      "Cost after iteration 11500: 0.715212\n",
      "Cost after iteration 12000: 0.710212\n",
      "Cost after iteration 12500: 0.706060\n",
      "Cost after iteration 13000: 0.702592\n",
      "Cost after iteration 13500: 0.699507\n",
      "Cost after iteration 14000: 0.696212\n",
      "Cost after iteration 14500: 0.692422\n",
      "Cost after iteration 15000: 0.688931\n",
      "Cost after iteration 15500: 0.685640\n",
      "Cost after iteration 16000: 0.682418\n",
      "Cost after iteration 16500: 0.679112\n",
      "Cost after iteration 17000: 0.675525\n",
      "Cost after iteration 17500: 0.671559\n",
      "Cost after iteration 18000: 0.667238\n",
      "Cost after iteration 18500: 0.662694\n",
      "Cost after iteration 19000: 0.657914\n",
      "Cost after iteration 19500: 0.653506\n",
      "------------------\n",
      "Train Accuracy:  89.1591382904795\n",
      "------------------\n",
      "Test Accuracy:  86.25\n",
      "------------------\n",
      "Fold:  7\n",
      "(1439, 11)\n",
      "(1439, 3)\n",
      "(160, 11)\n",
      "(160, 3)\n",
      "Cost after iteration 0: 7.508365\n",
      "Cost after iteration 500: 1.029042\n",
      "Cost after iteration 1000: 1.027263\n",
      "Cost after iteration 1500: 1.026802\n",
      "Cost after iteration 2000: 1.015842\n",
      "Cost after iteration 2500: 0.842649\n",
      "Cost after iteration 3000: 0.819637\n",
      "Cost after iteration 3500: 0.814273\n",
      "Cost after iteration 4000: 0.808777\n",
      "Cost after iteration 4500: 0.801312\n",
      "Cost after iteration 5000: 0.793360\n",
      "Cost after iteration 5500: 0.785418\n",
      "Cost after iteration 6000: 0.776674\n",
      "Cost after iteration 6500: 0.766942\n",
      "Cost after iteration 7000: 0.758141\n",
      "Cost after iteration 7500: 0.751543\n",
      "Cost after iteration 8000: 0.746420\n",
      "Cost after iteration 8500: 0.742215\n",
      "Cost after iteration 9000: 0.738438\n",
      "Cost after iteration 9500: 0.734616\n",
      "Cost after iteration 10000: 0.730445\n",
      "Cost after iteration 10500: 0.725743\n",
      "Cost after iteration 11000: 0.720503\n",
      "Cost after iteration 11500: 0.714951\n",
      "Cost after iteration 12000: 0.709411\n",
      "Cost after iteration 12500: 0.704116\n",
      "Cost after iteration 13000: 0.699227\n",
      "Cost after iteration 13500: 0.694904\n",
      "Cost after iteration 14000: 0.691162\n",
      "Cost after iteration 14500: 0.687825\n",
      "Cost after iteration 15000: 0.684698\n",
      "Cost after iteration 15500: 0.681646\n",
      "Cost after iteration 16000: 0.678591\n",
      "Cost after iteration 16500: 0.675493\n",
      "Cost after iteration 17000: 0.672351\n",
      "Cost after iteration 17500: 0.669194\n",
      "Cost after iteration 18000: 0.666068\n",
      "Cost after iteration 18500: 0.663013\n",
      "Cost after iteration 19000: 0.660044\n",
      "Cost after iteration 19500: 0.657147\n",
      "------------------\n",
      "Train Accuracy:  88.6726893676164\n",
      "------------------\n",
      "Test Accuracy:  84.375\n",
      "------------------\n",
      "Fold:  8\n",
      "(1440, 11)\n",
      "(1440, 3)\n",
      "(159, 11)\n",
      "(159, 3)\n",
      "Cost after iteration 0: 7.510881\n",
      "Cost after iteration 500: 1.030996\n",
      "Cost after iteration 1000: 1.029153\n",
      "Cost after iteration 1500: 1.028536\n",
      "Cost after iteration 2000: 1.006928\n",
      "Cost after iteration 2500: 0.841128\n",
      "Cost after iteration 3000: 0.824391\n",
      "Cost after iteration 3500: 0.819447\n",
      "Cost after iteration 4000: 0.814680\n",
      "Cost after iteration 4500: 0.808995\n",
      "Cost after iteration 5000: 0.803372\n",
      "Cost after iteration 5500: 0.798271\n",
      "Cost after iteration 6000: 0.793177\n",
      "Cost after iteration 6500: 0.787242\n",
      "Cost after iteration 7000: 0.779660\n",
      "Cost after iteration 7500: 0.772816\n",
      "Cost after iteration 8000: 0.767963\n",
      "Cost after iteration 8500: 0.763973\n",
      "Cost after iteration 9000: 0.760338\n",
      "Cost after iteration 9500: 0.756867\n",
      "Cost after iteration 10000: 0.753492\n",
      "Cost after iteration 10500: 0.750157\n",
      "Cost after iteration 11000: 0.746805\n",
      "Cost after iteration 11500: 0.743442\n",
      "Cost after iteration 12000: 0.740136\n",
      "Cost after iteration 12500: 0.736942\n",
      "Cost after iteration 13000: 0.733867\n",
      "Cost after iteration 13500: 0.730889\n",
      "Cost after iteration 14000: 0.727976\n",
      "Cost after iteration 14500: 0.725085\n",
      "Cost after iteration 15000: 0.722178\n",
      "Cost after iteration 15500: 0.719189\n",
      "Cost after iteration 16000: 0.716037\n",
      "Cost after iteration 16500: 0.712760\n",
      "Cost after iteration 17000: 0.709425\n",
      "Cost after iteration 17500: 0.706052\n",
      "Cost after iteration 18000: 0.702605\n",
      "Cost after iteration 18500: 0.698908\n",
      "Cost after iteration 19000: 0.694859\n",
      "Cost after iteration 19500: 0.690912\n",
      "------------------\n",
      "Train Accuracy:  88.19444444444444\n",
      "------------------\n",
      "Test Accuracy:  86.79245283018868\n",
      "------------------\n",
      "Fold:  9\n",
      "(1440, 11)\n",
      "(1440, 3)\n",
      "(159, 11)\n",
      "(159, 3)\n",
      "Cost after iteration 0: 7.513054\n",
      "Cost after iteration 500: 1.031009\n",
      "Cost after iteration 1000: 1.029175\n",
      "Cost after iteration 1500: 1.028677\n",
      "Cost after iteration 2000: 1.015941\n",
      "Cost after iteration 2500: 0.844232\n",
      "Cost after iteration 3000: 0.822184\n",
      "Cost after iteration 3500: 0.816634\n",
      "Cost after iteration 4000: 0.810581\n",
      "Cost after iteration 4500: 0.802948\n",
      "Cost after iteration 5000: 0.796521\n",
      "Cost after iteration 5500: 0.791305\n",
      "Cost after iteration 6000: 0.785601\n",
      "Cost after iteration 6500: 0.778178\n",
      "Cost after iteration 7000: 0.770105\n",
      "Cost after iteration 7500: 0.763952\n",
      "Cost after iteration 8000: 0.759031\n",
      "Cost after iteration 8500: 0.754542\n",
      "Cost after iteration 9000: 0.750260\n",
      "Cost after iteration 9500: 0.746129\n",
      "Cost after iteration 10000: 0.742158\n",
      "Cost after iteration 10500: 0.738356\n",
      "Cost after iteration 11000: 0.734704\n",
      "Cost after iteration 11500: 0.731169\n",
      "Cost after iteration 12000: 0.727729\n",
      "Cost after iteration 12500: 0.724382\n",
      "Cost after iteration 13000: 0.721144\n",
      "Cost after iteration 13500: 0.718047\n",
      "Cost after iteration 14000: 0.715133\n",
      "Cost after iteration 14500: 0.712420\n",
      "Cost after iteration 15000: 0.709849\n",
      "Cost after iteration 15500: 0.707298\n",
      "Cost after iteration 16000: 0.704657\n",
      "Cost after iteration 16500: 0.701909\n",
      "Cost after iteration 17000: 0.699131\n",
      "Cost after iteration 17500: 0.696379\n",
      "Cost after iteration 18000: 0.693557\n",
      "Cost after iteration 18500: 0.690470\n",
      "Cost after iteration 19000: 0.686854\n",
      "Cost after iteration 19500: 0.682878\n",
      "------------------\n",
      "Train Accuracy:  88.81944444444444\n",
      "------------------\n",
      "Test Accuracy:  84.27672955974843\n",
      "------------------\n",
      "Fold:  10\n",
      "(1441, 11)\n",
      "(1441, 3)\n",
      "(158, 11)\n",
      "(158, 3)\n",
      "Cost after iteration 0: 7.511524\n",
      "Cost after iteration 500: 1.030574\n",
      "Cost after iteration 1000: 1.028714\n",
      "Cost after iteration 1500: 1.028170\n",
      "Cost after iteration 2000: 1.011957\n",
      "Cost after iteration 2500: 0.838587\n",
      "Cost after iteration 3000: 0.816098\n",
      "Cost after iteration 3500: 0.810883\n",
      "Cost after iteration 4000: 0.806853\n",
      "Cost after iteration 4500: 0.802803\n",
      "Cost after iteration 5000: 0.797710\n",
      "Cost after iteration 5500: 0.792979\n",
      "Cost after iteration 6000: 0.788993\n",
      "Cost after iteration 6500: 0.785091\n",
      "Cost after iteration 7000: 0.780484\n",
      "Cost after iteration 7500: 0.774539\n",
      "Cost after iteration 8000: 0.767946\n",
      "Cost after iteration 8500: 0.762201\n",
      "Cost after iteration 9000: 0.757127\n",
      "Cost after iteration 9500: 0.752356\n",
      "Cost after iteration 10000: 0.747578\n",
      "Cost after iteration 10500: 0.742669\n",
      "Cost after iteration 11000: 0.737678\n",
      "Cost after iteration 11500: 0.732740\n",
      "Cost after iteration 12000: 0.727979\n",
      "Cost after iteration 12500: 0.723383\n",
      "Cost after iteration 13000: 0.718886\n",
      "Cost after iteration 13500: 0.714383\n",
      "Cost after iteration 14000: 0.709716\n",
      "Cost after iteration 14500: 0.704827\n",
      "Cost after iteration 15000: 0.699750\n",
      "Cost after iteration 15500: 0.694526\n",
      "Cost after iteration 16000: 0.689253\n",
      "Cost after iteration 16500: 0.684033\n",
      "Cost after iteration 17000: 0.678953\n",
      "Cost after iteration 17500: 0.674172\n",
      "Cost after iteration 18000: 0.669781\n",
      "Cost after iteration 18500: 0.665622\n",
      "Cost after iteration 19000: 0.661355\n",
      "Cost after iteration 19500: 0.656794\n",
      "------------------\n",
      "Train Accuracy:  87.78625954198473\n",
      "------------------\n",
      "Test Accuracy:  83.54430379746836\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "fold = 1\n",
    "layers_dims = [11, 10, 8, 5, 3] \n",
    "for train_index, test_index in skf.split(X_index, Y_index):\n",
    "    print(\"Fold: \",fold)\n",
    "    fold += 1\n",
    "\n",
    "    X_train, X_test = XX[train_index], XX[test_index]\n",
    "    Y_train, Y_test = YY[train_index], YY[test_index]\n",
    "    \n",
    "    parameters = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, layers_dims, learning_rate = 0.99, num_iterations = 20000, print_cost = True)\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Falta esccarlar sobre todos los datos :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
