{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import minmax_scale as mms\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,name):\n",
    "    csv_path = os.path.join(path, name)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading music dataset ... (1059, 70)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.161286</td>\n",
       "      <td>7.835325</td>\n",
       "      <td>2.911583</td>\n",
       "      <td>0.984049</td>\n",
       "      <td>-1.499546</td>\n",
       "      <td>-2.094097</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>-1.205671</td>\n",
       "      <td>1.849122</td>\n",
       "      <td>-0.425598</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.504263</td>\n",
       "      <td>0.351267</td>\n",
       "      <td>-1.018726</td>\n",
       "      <td>-0.174878</td>\n",
       "      <td>-1.089543</td>\n",
       "      <td>-0.668840</td>\n",
       "      <td>-0.914772</td>\n",
       "      <td>-0.836250</td>\n",
       "      <td>-15.75</td>\n",
       "      <td>-47.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.225763</td>\n",
       "      <td>-0.094169</td>\n",
       "      <td>-0.603646</td>\n",
       "      <td>0.497745</td>\n",
       "      <td>0.874036</td>\n",
       "      <td>0.290280</td>\n",
       "      <td>-0.077659</td>\n",
       "      <td>-0.887385</td>\n",
       "      <td>0.432062</td>\n",
       "      <td>-0.093963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.495712</td>\n",
       "      <td>-0.465077</td>\n",
       "      <td>-0.157861</td>\n",
       "      <td>-0.157189</td>\n",
       "      <td>0.380951</td>\n",
       "      <td>1.088478</td>\n",
       "      <td>-0.123595</td>\n",
       "      <td>1.391141</td>\n",
       "      <td>14.91</td>\n",
       "      <td>-23.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.692525</td>\n",
       "      <td>-0.517801</td>\n",
       "      <td>-0.788035</td>\n",
       "      <td>1.214351</td>\n",
       "      <td>-0.907214</td>\n",
       "      <td>0.880213</td>\n",
       "      <td>0.406899</td>\n",
       "      <td>-0.694895</td>\n",
       "      <td>-0.901869</td>\n",
       "      <td>-1.701574</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.637167</td>\n",
       "      <td>0.147260</td>\n",
       "      <td>0.217914</td>\n",
       "      <td>2.718442</td>\n",
       "      <td>0.972919</td>\n",
       "      <td>2.081069</td>\n",
       "      <td>1.375763</td>\n",
       "      <td>1.063847</td>\n",
       "      <td>12.65</td>\n",
       "      <td>-8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.735562</td>\n",
       "      <td>-0.684055</td>\n",
       "      <td>2.058215</td>\n",
       "      <td>0.716328</td>\n",
       "      <td>-0.011393</td>\n",
       "      <td>0.805396</td>\n",
       "      <td>1.497982</td>\n",
       "      <td>0.114752</td>\n",
       "      <td>0.692847</td>\n",
       "      <td>0.052377</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178325</td>\n",
       "      <td>-0.065059</td>\n",
       "      <td>-0.724247</td>\n",
       "      <td>-1.020687</td>\n",
       "      <td>-0.751380</td>\n",
       "      <td>-0.385005</td>\n",
       "      <td>-0.012326</td>\n",
       "      <td>-0.392197</td>\n",
       "      <td>9.03</td>\n",
       "      <td>38.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.570272</td>\n",
       "      <td>0.273157</td>\n",
       "      <td>-0.279214</td>\n",
       "      <td>0.083456</td>\n",
       "      <td>1.049331</td>\n",
       "      <td>-0.869295</td>\n",
       "      <td>-0.265858</td>\n",
       "      <td>-0.401676</td>\n",
       "      <td>-0.872639</td>\n",
       "      <td>1.147483</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.919463</td>\n",
       "      <td>-0.667912</td>\n",
       "      <td>-0.820172</td>\n",
       "      <td>-0.190488</td>\n",
       "      <td>0.306974</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>0.271838</td>\n",
       "      <td>1.289783</td>\n",
       "      <td>34.03</td>\n",
       "      <td>-6.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  7.161286  7.835325  2.911583  0.984049 -1.499546 -2.094097  0.576000   \n",
       "1  0.225763 -0.094169 -0.603646  0.497745  0.874036  0.290280 -0.077659   \n",
       "2 -0.692525 -0.517801 -0.788035  1.214351 -0.907214  0.880213  0.406899   \n",
       "3 -0.735562 -0.684055  2.058215  0.716328 -0.011393  0.805396  1.497982   \n",
       "4  0.570272  0.273157 -0.279214  0.083456  1.049331 -0.869295 -0.265858   \n",
       "\n",
       "          7         8         9  ...        60        61        62        63  \\\n",
       "0 -1.205671  1.849122 -0.425598  ... -1.504263  0.351267 -1.018726 -0.174878   \n",
       "1 -0.887385  0.432062 -0.093963  ... -0.495712 -0.465077 -0.157861 -0.157189   \n",
       "2 -0.694895 -0.901869 -1.701574  ... -0.637167  0.147260  0.217914  2.718442   \n",
       "3  0.114752  0.692847  0.052377  ... -0.178325 -0.065059 -0.724247 -1.020687   \n",
       "4 -0.401676 -0.872639  1.147483  ... -0.919463 -0.667912 -0.820172 -0.190488   \n",
       "\n",
       "         64        65        66        67     68     69  \n",
       "0 -1.089543 -0.668840 -0.914772 -0.836250 -15.75 -47.95  \n",
       "1  0.380951  1.088478 -0.123595  1.391141  14.91 -23.51  \n",
       "2  0.972919  2.081069  1.375763  1.063847  12.65  -8.00  \n",
       "3 -0.751380 -0.385005 -0.012326 -0.392197   9.03  38.74  \n",
       "4  0.306974  0.119658  0.271838  1.289783  34.03  -6.85  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"C:/Users/DanielaFe7/Desktop/Maestrado/Redes/RedesNeurais\"\n",
    "name = \"default_features_1059_tracks.txt\"\n",
    "data = load_data(path,name)\n",
    "print(\"Loading music dataset ...\",data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalling the data ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Scalling the data ...\")\n",
    "def scalling(data):\n",
    "    data_scalling = mms(data)\n",
    "    return data_scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN L-Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    S = 1 / (1 + np.exp(-Z))\n",
    "    return S,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivade_sigmoid(z):\n",
    "    gz,z = sigmoid(z) \n",
    "    return gz * (1-gz);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network, (including input layer)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.normal(0, 1, (layer_dims[l], layer_dims[l-1]))\n",
    "        parameters['b' + str(l)] = np.random.random((layer_dims[l], 1))\n",
    "      \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks (not input layer including)\n",
    "    v = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l+1)])\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l+1)])\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(Y_pred, Y):\n",
    "    Y = Y.T\n",
    "    Y_pred = Y_pred.T\n",
    "    #print(\"RMSE: \", Y_pred.shape)\n",
    "    rmse = (Y - Y_pred)**2\n",
    "    rmse = np.sum(rmse, axis=0)\n",
    "    return rmse / Y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation in L - Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache) #linear_cahce: A_prev,Wi,bi - activation_cache: Zi\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X,Y,parameters,pred=False):\n",
    "    caches = []\n",
    "    A = X                                     #(input size, number of examples)\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network (not input layer including)\n",
    "    \n",
    "    # Implement [LINEAR -> SIGMOID]*(L-1). To L-1 Layers\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        W=parameters['W' + str(l)]\n",
    "        b=parameters['b' + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, \"sigmoid\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Last layer\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    # Predicction\n",
    "    if pred:\n",
    "        # In predicction Task, we keep the same values\n",
    "        return AL\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (- 1 / m) * np.sum(Y * np.log(AL) + (1 - Y) * (np.log(1 - AL)))\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation in L - Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m) * np.dot(dZ,A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = dA * derivade_sigmoid(activation_cache) # activation_cache = Z ; dA = np.dot(W.T,dZ) excep the first dA\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers (not input layer including)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL =- (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, \n",
    "                                                                                                  current_cache, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (SIGMOID -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev, dW, db = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"sigmoid\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev\n",
    "        grads[\"dW\" + str(l + 1)] = dW\n",
    "        grads[\"db\" + str(l + 1)] = db\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network (not input layer including)\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Parameters with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, learning_rate, beta):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks (not input layer including)\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads['dW' + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads['db' + str(l + 1)]\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L - layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = []\n",
    "def L_layer_model(X, Y, X_test, Y_test, layers_dims, learning_rate, num_iterations, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []                         \n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    #Velocity initialization.\n",
    "    v = initialize_velocity(parameters)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> SIGMOID]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X,Y,parameters,pred=False)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters. #Esto tem que quedar comentado si vc vai usar com Momemtum\n",
    "        #parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Update parameters with Momentum.\n",
    "        parameters, v = update_parameters_with_momentum(parameters, grads, v, learning_rate, beta = 0.99)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 500 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 500 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # Train Predicction\n",
    "    Y_prediction =  L_model_forward(X,Y,parameters,pred=True)\n",
    "    print(\"------------------\")\n",
    "    #print(\"Train Prediccion:\")\n",
    "    #print(Y_prediction.shape)\n",
    "    #print(Y_prediction)\n",
    "    print(\"Train RMSE: \",RMSE(Y_prediction, Y))  \n",
    "    \n",
    "    # Test Predicction\n",
    "    print(\"------------------\")\n",
    "    Y_prediction =  L_model_forward(X_test,Y_test,parameters,pred=True)\n",
    "    #print(\"Test Prediccion:\")\n",
    "    #print(Y_prediction.shape)\n",
    "    #print(Y_prediction)\n",
    "    print(\"Test RMSE: \",RMSE(Y_prediction, Y_test))  \n",
    "    \n",
    "        \n",
    "    # plot the cost\n",
    "    #plt.plot(np.squeeze(costs))\n",
    "    #plt.ylabel('cost')\n",
    "    #plt.xlabel('iterations (per tens)')\n",
    "    #plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    #plt.show()\n",
    "            \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation: KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1059, 68)\n",
      "(1059, 2)\n",
      "KFold(n_splits=10, random_state=2, shuffle=True)\n",
      "(1059, 68)\n",
      "(1059, 2)\n"
     ]
    }
   ],
   "source": [
    "# We can not use StratifiedKFold because is an Regression problem and we have not labels, we need to scale the hole data then.\n",
    "X_index = data\n",
    "Y_index = data[[\"68\",\"69\"]]\n",
    "X_index = X_index.drop(\"68\", axis = 1)\n",
    "X_index = X_index.drop(\"69\", axis = 1)\n",
    "print(X_index.shape)\n",
    "print(Y_index.shape)\n",
    "kf = KFold(n_splits=10,random_state=2,shuffle=True)\n",
    "kf.get_n_splits(X_index)\n",
    "print(kf)\n",
    "\n",
    "# When the indexes are ready in (split function), pick the data that is already pre-processing\n",
    "scaling_data = scalling(data)\n",
    "XX = scaling_data\n",
    "XX = np.delete(XX, np.s_[-1], axis=1)\n",
    "XX = np.delete(XX, np.s_[-1], axis=1)\n",
    "YY = scaling_data[:,-2:]\n",
    "print(XX.shape)\n",
    "print(YY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "Cost after iteration 0: 1.467737\n",
      "Cost after iteration 500: 1.304763\n",
      "Cost after iteration 1000: 1.295018\n",
      "Cost after iteration 1500: 1.287046\n",
      "Cost after iteration 2000: 1.282399\n",
      "Cost after iteration 2500: 1.278571\n",
      "Cost after iteration 3000: 1.272632\n",
      "Cost after iteration 3500: 1.265089\n",
      "Cost after iteration 4000: 1.259544\n",
      "Cost after iteration 4500: 1.255194\n",
      "Cost after iteration 5000: 1.251434\n",
      "Cost after iteration 5500: 1.248067\n",
      "Cost after iteration 6000: 1.244989\n",
      "Cost after iteration 6500: 1.242165\n",
      "Cost after iteration 7000: 1.239588\n",
      "Cost after iteration 7500: 1.237228\n",
      "Cost after iteration 8000: 1.235029\n",
      "Cost after iteration 8500: 1.232940\n",
      "Cost after iteration 9000: 1.230916\n",
      "Cost after iteration 9500: 1.228917\n",
      "Cost after iteration 10000: 1.226919\n",
      "Cost after iteration 10500: 1.224917\n",
      "Cost after iteration 11000: 1.222926\n",
      "Cost after iteration 11500: 1.220970\n",
      "Cost after iteration 12000: 1.219057\n",
      "Cost after iteration 12500: 1.217184\n",
      "Cost after iteration 13000: 1.215339\n",
      "Cost after iteration 13500: 1.213514\n",
      "Cost after iteration 14000: 1.211711\n",
      "Cost after iteration 14500: 1.209932\n",
      "Cost after iteration 15000: 1.208181\n",
      "Cost after iteration 15500: 1.206474\n",
      "Cost after iteration 16000: 1.204837\n",
      "Cost after iteration 16500: 1.203284\n",
      "Cost after iteration 17000: 1.201806\n",
      "Cost after iteration 17500: 1.200389\n",
      "Cost after iteration 18000: 1.199020\n",
      "Cost after iteration 18500: 1.197689\n",
      "Cost after iteration 19000: 1.196393\n",
      "Cost after iteration 19500: 1.195133\n",
      "------------------\n",
      "Train RMSE:  [0.01984054 0.01781765]\n",
      "------------------\n",
      "Test RMSE:  [0.03861978 0.03742349]\n",
      "------------------\n",
      "Fold:  2\n",
      "Cost after iteration 0: 1.466320\n",
      "Cost after iteration 500: 1.304899\n",
      "Cost after iteration 1000: 1.296124\n",
      "Cost after iteration 1500: 1.288970\n",
      "Cost after iteration 2000: 1.284233\n",
      "Cost after iteration 2500: 1.279509\n",
      "Cost after iteration 3000: 1.271253\n",
      "Cost after iteration 3500: 1.264448\n",
      "Cost after iteration 4000: 1.259912\n",
      "Cost after iteration 4500: 1.256195\n",
      "Cost after iteration 5000: 1.252944\n",
      "Cost after iteration 5500: 1.250088\n",
      "Cost after iteration 6000: 1.247584\n",
      "Cost after iteration 6500: 1.245363\n",
      "Cost after iteration 7000: 1.243344\n",
      "Cost after iteration 7500: 1.241455\n",
      "Cost after iteration 8000: 1.239635\n",
      "Cost after iteration 8500: 1.237840\n",
      "Cost after iteration 9000: 1.236035\n",
      "Cost after iteration 9500: 1.234186\n",
      "Cost after iteration 10000: 1.232254\n",
      "Cost after iteration 10500: 1.230193\n",
      "Cost after iteration 11000: 1.227970\n",
      "Cost after iteration 11500: 1.225583\n",
      "Cost after iteration 12000: 1.223074\n",
      "Cost after iteration 12500: 1.220519\n",
      "Cost after iteration 13000: 1.217991\n",
      "Cost after iteration 13500: 1.215533\n",
      "Cost after iteration 14000: 1.213180\n",
      "Cost after iteration 14500: 1.210971\n",
      "Cost after iteration 15000: 1.208936\n",
      "Cost after iteration 15500: 1.207081\n",
      "Cost after iteration 16000: 1.205386\n",
      "Cost after iteration 16500: 1.203825\n",
      "Cost after iteration 17000: 1.202374\n",
      "Cost after iteration 17500: 1.201012\n",
      "Cost after iteration 18000: 1.199721\n",
      "Cost after iteration 18500: 1.198489\n",
      "Cost after iteration 19000: 1.197302\n",
      "Cost after iteration 19500: 1.196148\n",
      "------------------\n",
      "Train RMSE:  [0.01898446 0.01736327]\n",
      "------------------\n",
      "Test RMSE:  [0.05442359 0.04061039]\n",
      "------------------\n",
      "Fold:  3\n",
      "Cost after iteration 0: 1.465718\n",
      "Cost after iteration 500: 1.307376\n",
      "Cost after iteration 1000: 1.298411\n",
      "Cost after iteration 1500: 1.290812\n",
      "Cost after iteration 2000: 1.286226\n",
      "Cost after iteration 2500: 1.282312\n",
      "Cost after iteration 3000: 1.274811\n",
      "Cost after iteration 3500: 1.268087\n",
      "Cost after iteration 4000: 1.263623\n",
      "Cost after iteration 4500: 1.260192\n",
      "Cost after iteration 5000: 1.257181\n",
      "Cost after iteration 5500: 1.254425\n",
      "Cost after iteration 6000: 1.251913\n",
      "Cost after iteration 6500: 1.249650\n",
      "Cost after iteration 7000: 1.247564\n",
      "Cost after iteration 7500: 1.245574\n",
      "Cost after iteration 8000: 1.243627\n",
      "Cost after iteration 8500: 1.241672\n",
      "Cost after iteration 9000: 1.239679\n",
      "Cost after iteration 9500: 1.237639\n",
      "Cost after iteration 10000: 1.235548\n",
      "Cost after iteration 10500: 1.233404\n",
      "Cost after iteration 11000: 1.231222\n",
      "Cost after iteration 11500: 1.229027\n",
      "Cost after iteration 12000: 1.226845\n",
      "Cost after iteration 12500: 1.224684\n",
      "Cost after iteration 13000: 1.222539\n",
      "Cost after iteration 13500: 1.220400\n",
      "Cost after iteration 14000: 1.218260\n",
      "Cost after iteration 14500: 1.216123\n",
      "Cost after iteration 15000: 1.214004\n",
      "Cost after iteration 15500: 1.211928\n",
      "Cost after iteration 16000: 1.209927\n",
      "Cost after iteration 16500: 1.208023\n",
      "Cost after iteration 17000: 1.206215\n",
      "Cost after iteration 17500: 1.204490\n",
      "Cost after iteration 18000: 1.202835\n",
      "Cost after iteration 18500: 1.201242\n",
      "Cost after iteration 19000: 1.199704\n",
      "Cost after iteration 19500: 1.198216\n",
      "------------------\n",
      "Train RMSE:  [0.01781744 0.0194558 ]\n",
      "------------------\n",
      "Test RMSE:  [0.03456296 0.03120313]\n",
      "------------------\n",
      "Fold:  4\n",
      "Cost after iteration 0: 1.467841\n",
      "Cost after iteration 500: 1.302378\n",
      "Cost after iteration 1000: 1.293079\n",
      "Cost after iteration 1500: 1.285385\n",
      "Cost after iteration 2000: 1.281024\n",
      "Cost after iteration 2500: 1.278002\n",
      "Cost after iteration 3000: 1.273548\n",
      "Cost after iteration 3500: 1.265895\n",
      "Cost after iteration 4000: 1.259767\n",
      "Cost after iteration 4500: 1.255526\n",
      "Cost after iteration 5000: 1.251962\n",
      "Cost after iteration 5500: 1.248769\n",
      "Cost after iteration 6000: 1.245902\n",
      "Cost after iteration 6500: 1.243325\n",
      "Cost after iteration 7000: 1.240944\n",
      "Cost after iteration 7500: 1.238650\n",
      "Cost after iteration 8000: 1.236367\n",
      "Cost after iteration 8500: 1.234073\n",
      "Cost after iteration 9000: 1.231804\n",
      "Cost after iteration 9500: 1.229607\n",
      "Cost after iteration 10000: 1.227495\n",
      "Cost after iteration 10500: 1.225450\n",
      "Cost after iteration 11000: 1.223441\n",
      "Cost after iteration 11500: 1.221436\n",
      "Cost after iteration 12000: 1.219411\n",
      "Cost after iteration 12500: 1.217368\n",
      "Cost after iteration 13000: 1.215329\n",
      "Cost after iteration 13500: 1.213321\n",
      "Cost after iteration 14000: 1.211357\n",
      "Cost after iteration 14500: 1.209437\n",
      "Cost after iteration 15000: 1.207556\n",
      "Cost after iteration 15500: 1.205714\n",
      "Cost after iteration 16000: 1.203922\n",
      "Cost after iteration 16500: 1.202196\n",
      "Cost after iteration 17000: 1.200549\n",
      "Cost after iteration 17500: 1.198986\n",
      "Cost after iteration 18000: 1.197502\n",
      "Cost after iteration 18500: 1.196087\n",
      "Cost after iteration 19000: 1.194731\n",
      "Cost after iteration 19500: 1.193423\n",
      "------------------\n",
      "Train RMSE:  [0.01899642 0.01717278]\n",
      "------------------\n",
      "Test RMSE:  [0.05352471 0.04379078]\n",
      "------------------\n",
      "Fold:  5\n",
      "Cost after iteration 0: 1.466588\n",
      "Cost after iteration 500: 1.304766\n",
      "Cost after iteration 1000: 1.294963\n",
      "Cost after iteration 1500: 1.287318\n",
      "Cost after iteration 2000: 1.283364\n",
      "Cost after iteration 2500: 1.280101\n",
      "Cost after iteration 3000: 1.274946\n",
      "Cost after iteration 3500: 1.267531\n",
      "Cost after iteration 4000: 1.262110\n",
      "Cost after iteration 4500: 1.257730\n",
      "Cost after iteration 5000: 1.254042\n",
      "Cost after iteration 5500: 1.250914\n",
      "Cost after iteration 6000: 1.248224\n",
      "Cost after iteration 6500: 1.245864\n",
      "Cost after iteration 7000: 1.243739\n",
      "Cost after iteration 7500: 1.241792\n",
      "Cost after iteration 8000: 1.239976\n",
      "Cost after iteration 8500: 1.238250\n",
      "Cost after iteration 9000: 1.236594\n",
      "Cost after iteration 9500: 1.235000\n",
      "Cost after iteration 10000: 1.233465\n",
      "Cost after iteration 10500: 1.231979\n",
      "Cost after iteration 11000: 1.230527\n",
      "Cost after iteration 11500: 1.229093\n",
      "Cost after iteration 12000: 1.227663\n",
      "Cost after iteration 12500: 1.226231\n",
      "Cost after iteration 13000: 1.224799\n",
      "Cost after iteration 13500: 1.223367\n",
      "Cost after iteration 14000: 1.221940\n",
      "Cost after iteration 14500: 1.220517\n",
      "Cost after iteration 15000: 1.219099\n",
      "Cost after iteration 15500: 1.217683\n",
      "Cost after iteration 16000: 1.216264\n",
      "Cost after iteration 16500: 1.214836\n",
      "Cost after iteration 17000: 1.213394\n",
      "Cost after iteration 17500: 1.211934\n",
      "Cost after iteration 18000: 1.210460\n",
      "Cost after iteration 18500: 1.208981\n",
      "Cost after iteration 19000: 1.207508\n",
      "Cost after iteration 19500: 1.206057\n",
      "------------------\n",
      "Train RMSE:  [0.02096452 0.01871201]\n",
      "------------------\n",
      "Test RMSE:  [0.03327979 0.04430754]\n",
      "------------------\n",
      "Fold:  6\n",
      "Cost after iteration 0: 1.462987\n",
      "Cost after iteration 500: 1.308378\n",
      "Cost after iteration 1000: 1.299842\n",
      "Cost after iteration 1500: 1.292091\n",
      "Cost after iteration 2000: 1.287470\n",
      "Cost after iteration 2500: 1.284070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 3000: 1.278800\n",
      "Cost after iteration 3500: 1.270877\n",
      "Cost after iteration 4000: 1.265263\n",
      "Cost after iteration 4500: 1.261281\n",
      "Cost after iteration 5000: 1.258040\n",
      "Cost after iteration 5500: 1.255247\n",
      "Cost after iteration 6000: 1.252772\n",
      "Cost after iteration 6500: 1.250533\n",
      "Cost after iteration 7000: 1.248448\n",
      "Cost after iteration 7500: 1.246428\n",
      "Cost after iteration 8000: 1.244406\n",
      "Cost after iteration 8500: 1.242341\n",
      "Cost after iteration 9000: 1.240239\n",
      "Cost after iteration 9500: 1.238136\n",
      "Cost after iteration 10000: 1.236058\n",
      "Cost after iteration 10500: 1.234012\n",
      "Cost after iteration 11000: 1.231999\n",
      "Cost after iteration 11500: 1.230028\n",
      "Cost after iteration 12000: 1.228111\n",
      "Cost after iteration 12500: 1.226255\n",
      "Cost after iteration 13000: 1.224462\n",
      "Cost after iteration 13500: 1.222731\n",
      "Cost after iteration 14000: 1.221066\n",
      "Cost after iteration 14500: 1.219477\n",
      "Cost after iteration 15000: 1.217973\n",
      "Cost after iteration 15500: 1.216555\n",
      "Cost after iteration 16000: 1.215215\n",
      "Cost after iteration 16500: 1.213941\n",
      "Cost after iteration 17000: 1.212718\n",
      "Cost after iteration 17500: 1.211538\n",
      "Cost after iteration 18000: 1.210391\n",
      "Cost after iteration 18500: 1.209271\n",
      "Cost after iteration 19000: 1.208173\n",
      "Cost after iteration 19500: 1.207092\n",
      "------------------\n",
      "Train RMSE:  [0.01795857 0.02180889]\n",
      "------------------\n",
      "Test RMSE:  [0.04938827 0.03957727]\n",
      "------------------\n",
      "Fold:  7\n",
      "Cost after iteration 0: 1.464997\n",
      "Cost after iteration 500: 1.306413\n",
      "Cost after iteration 1000: 1.297236\n",
      "Cost after iteration 1500: 1.289443\n",
      "Cost after iteration 2000: 1.284480\n",
      "Cost after iteration 2500: 1.279520\n",
      "Cost after iteration 3000: 1.271352\n",
      "Cost after iteration 3500: 1.264577\n",
      "Cost after iteration 4000: 1.259970\n",
      "Cost after iteration 4500: 1.256193\n",
      "Cost after iteration 5000: 1.252867\n",
      "Cost after iteration 5500: 1.249889\n",
      "Cost after iteration 6000: 1.247161\n",
      "Cost after iteration 6500: 1.244581\n",
      "Cost after iteration 7000: 1.242080\n",
      "Cost after iteration 7500: 1.239633\n",
      "Cost after iteration 8000: 1.237236\n",
      "Cost after iteration 8500: 1.234902\n",
      "Cost after iteration 9000: 1.232640\n",
      "Cost after iteration 9500: 1.230440\n",
      "Cost after iteration 10000: 1.228301\n",
      "Cost after iteration 10500: 1.226205\n",
      "Cost after iteration 11000: 1.224140\n",
      "Cost after iteration 11500: 1.222111\n",
      "Cost after iteration 12000: 1.220130\n",
      "Cost after iteration 12500: 1.218214\n",
      "Cost after iteration 13000: 1.216371\n",
      "Cost after iteration 13500: 1.214604\n",
      "Cost after iteration 14000: 1.212914\n",
      "Cost after iteration 14500: 1.211293\n",
      "Cost after iteration 15000: 1.209731\n",
      "Cost after iteration 15500: 1.208216\n",
      "Cost after iteration 16000: 1.206733\n",
      "Cost after iteration 16500: 1.205275\n",
      "Cost after iteration 17000: 1.203839\n",
      "Cost after iteration 17500: 1.202424\n",
      "Cost after iteration 18000: 1.201034\n",
      "Cost after iteration 18500: 1.199668\n",
      "Cost after iteration 19000: 1.198325\n",
      "Cost after iteration 19500: 1.197002\n",
      "------------------\n",
      "Train RMSE:  [0.01805435 0.01900815]\n",
      "------------------\n",
      "Test RMSE:  [0.03944309 0.03392119]\n",
      "------------------\n",
      "Fold:  8\n",
      "Cost after iteration 0: 1.465296\n",
      "Cost after iteration 500: 1.306493\n",
      "Cost after iteration 1000: 1.297674\n",
      "Cost after iteration 1500: 1.290251\n",
      "Cost after iteration 2000: 1.285500\n",
      "Cost after iteration 2500: 1.280750\n",
      "Cost after iteration 3000: 1.272679\n",
      "Cost after iteration 3500: 1.265410\n",
      "Cost after iteration 4000: 1.260133\n",
      "Cost after iteration 4500: 1.255953\n",
      "Cost after iteration 5000: 1.252518\n",
      "Cost after iteration 5500: 1.249597\n",
      "Cost after iteration 6000: 1.247027\n",
      "Cost after iteration 6500: 1.244690\n",
      "Cost after iteration 7000: 1.242458\n",
      "Cost after iteration 7500: 1.240215\n",
      "Cost after iteration 8000: 1.237883\n",
      "Cost after iteration 8500: 1.235434\n",
      "Cost after iteration 9000: 1.232864\n",
      "Cost after iteration 9500: 1.230186\n",
      "Cost after iteration 10000: 1.227450\n",
      "Cost after iteration 10500: 1.224723\n",
      "Cost after iteration 11000: 1.222056\n",
      "Cost after iteration 11500: 1.219490\n",
      "Cost after iteration 12000: 1.217049\n",
      "Cost after iteration 12500: 1.214737\n",
      "Cost after iteration 13000: 1.212537\n",
      "Cost after iteration 13500: 1.210419\n",
      "Cost after iteration 14000: 1.208357\n",
      "Cost after iteration 14500: 1.206342\n",
      "Cost after iteration 15000: 1.204385\n",
      "Cost after iteration 15500: 1.202497\n",
      "Cost after iteration 16000: 1.200680\n",
      "Cost after iteration 16500: 1.198929\n",
      "Cost after iteration 17000: 1.197238\n",
      "Cost after iteration 17500: 1.195610\n",
      "Cost after iteration 18000: 1.194050\n",
      "Cost after iteration 18500: 1.192567\n",
      "Cost after iteration 19000: 1.191166\n",
      "Cost after iteration 19500: 1.189851\n",
      "------------------\n",
      "Train RMSE:  [0.01558002 0.01847381]\n",
      "------------------\n",
      "Test RMSE:  [0.0495467  0.04518123]\n",
      "------------------\n",
      "Fold:  9\n",
      "Cost after iteration 0: 1.468664\n",
      "Cost after iteration 500: 1.302614\n",
      "Cost after iteration 1000: 1.292426\n",
      "Cost after iteration 1500: 1.285256\n",
      "Cost after iteration 2000: 1.281231\n",
      "Cost after iteration 2500: 1.276989\n",
      "Cost after iteration 3000: 1.269145\n",
      "Cost after iteration 3500: 1.262765\n",
      "Cost after iteration 4000: 1.258056\n",
      "Cost after iteration 4500: 1.254230\n",
      "Cost after iteration 5000: 1.250953\n",
      "Cost after iteration 5500: 1.248104\n",
      "Cost after iteration 6000: 1.245601\n",
      "Cost after iteration 6500: 1.243362\n",
      "Cost after iteration 7000: 1.241295\n",
      "Cost after iteration 7500: 1.239305\n",
      "Cost after iteration 8000: 1.237300\n",
      "Cost after iteration 8500: 1.235215\n",
      "Cost after iteration 9000: 1.233090\n",
      "Cost after iteration 9500: 1.230997\n",
      "Cost after iteration 10000: 1.228924\n",
      "Cost after iteration 10500: 1.226831\n",
      "Cost after iteration 11000: 1.224680\n",
      "Cost after iteration 11500: 1.222474\n",
      "Cost after iteration 12000: 1.220247\n",
      "Cost after iteration 12500: 1.218032\n",
      "Cost after iteration 13000: 1.215859\n",
      "Cost after iteration 13500: 1.213746\n",
      "Cost after iteration 14000: 1.211691\n",
      "Cost after iteration 14500: 1.209688\n",
      "Cost after iteration 15000: 1.207730\n",
      "Cost after iteration 15500: 1.205814\n",
      "Cost after iteration 16000: 1.203933\n",
      "Cost after iteration 16500: 1.202081\n",
      "Cost after iteration 17000: 1.200265\n",
      "Cost after iteration 17500: 1.198505\n",
      "Cost after iteration 18000: 1.196830\n",
      "Cost after iteration 18500: 1.195265\n",
      "Cost after iteration 19000: 1.193823\n",
      "Cost after iteration 19500: 1.192498\n",
      "------------------\n",
      "Train RMSE:  [0.01681678 0.01912787]\n",
      "------------------\n",
      "Test RMSE:  [0.04103603 0.04554431]\n",
      "------------------\n",
      "Fold:  10\n",
      "Cost after iteration 0: 1.464032\n",
      "Cost after iteration 500: 1.306897\n",
      "Cost after iteration 1000: 1.297600\n",
      "Cost after iteration 1500: 1.290199\n",
      "Cost after iteration 2000: 1.285758\n",
      "Cost after iteration 2500: 1.281873\n",
      "Cost after iteration 3000: 1.275176\n",
      "Cost after iteration 3500: 1.267791\n",
      "Cost after iteration 4000: 1.262820\n",
      "Cost after iteration 4500: 1.258846\n",
      "Cost after iteration 5000: 1.255294\n",
      "Cost after iteration 5500: 1.252082\n",
      "Cost after iteration 6000: 1.249215\n",
      "Cost after iteration 6500: 1.246641\n",
      "Cost after iteration 7000: 1.244264\n",
      "Cost after iteration 7500: 1.242001\n",
      "Cost after iteration 8000: 1.239808\n",
      "Cost after iteration 8500: 1.237672\n",
      "Cost after iteration 9000: 1.235578\n",
      "Cost after iteration 9500: 1.233498\n",
      "Cost after iteration 10000: 1.231392\n",
      "Cost after iteration 10500: 1.229224\n",
      "Cost after iteration 11000: 1.226964\n",
      "Cost after iteration 11500: 1.224591\n",
      "Cost after iteration 12000: 1.222092\n",
      "Cost after iteration 12500: 1.219487\n",
      "Cost after iteration 13000: 1.216845\n",
      "Cost after iteration 13500: 1.214256\n",
      "Cost after iteration 14000: 1.211788\n",
      "Cost after iteration 14500: 1.209453\n",
      "Cost after iteration 15000: 1.207233\n",
      "Cost after iteration 15500: 1.205118\n",
      "Cost after iteration 16000: 1.203116\n",
      "Cost after iteration 16500: 1.201241\n",
      "Cost after iteration 17000: 1.199501\n",
      "Cost after iteration 17500: 1.197891\n",
      "Cost after iteration 18000: 1.196395\n",
      "Cost after iteration 18500: 1.194995\n",
      "Cost after iteration 19000: 1.193671\n",
      "Cost after iteration 19500: 1.192406\n",
      "------------------\n",
      "Train RMSE:  [0.01733893 0.01884535]\n",
      "------------------\n",
      "Test RMSE:  [0.03770918 0.0348751 ]\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "fold = 1\n",
    "layers_dims = [68, 10, 8, 5, 2] \n",
    "for train_index, test_index in kf.split(X_index):\n",
    "    print(\"Fold: \",fold)\n",
    "    fold += 1\n",
    "   \n",
    "    X_train, X_test = XX[train_index], XX[test_index]\n",
    "    Y_train, Y_test = YY[train_index], YY[test_index]\n",
    "    \n",
    "    parameters = L_layer_model(X_train.T, Y_train.T, X_test.T, Y_test.T, layers_dims, learning_rate = 0.99, num_iterations = 20000, print_cost = True)\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
